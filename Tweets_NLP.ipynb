{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HackE_Tweets_NLP",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNwLseno_46j",
        "colab_type": "text"
      },
      "source": [
        "**THIS IS A SUBMISSION MADE TO THE \"HACKEREARTH MACHINE LEARNING CONTEST\" ON MOTHER'S DAY RELATED TWEETS SENTIMENT CLASSIFICATION FOR THREE CLASSES (POSITIVE , NEGATIVE AND NEUTRAL)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH9ba4T4-4HM",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTING DATA USING PANDAS**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNeqdkeKBrzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train_data_path=\"\" #Insert your own path here\n",
        "train_data=pd.read_csv('train_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsx4TlMdL0yO",
        "colab_type": "code",
        "outputId": "610d125f-0f92-4c08-f139-61075fba493d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "df1=train_data.groupby(train_data.sentiment_class).nunique()\n",
        "print(df1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>sentiment_class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment_class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>-1</th>\n",
              "      <td>769</td>\n",
              "      <td>769</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1701</td>\n",
              "      <td>1701</td>\n",
              "      <td>231</td>\n",
              "      <td>61</td>\n",
              "      <td>891</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>765</td>\n",
              "      <td>765</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>479</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  original_text  ...  original_author  sentiment_class\n",
              "sentiment_class                       ...                                  \n",
              "-1                769            769  ...              497                1\n",
              " 0               1701           1701  ...              891                1\n",
              " 1                765            765  ...              479                1\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdWrzs8F_WY4",
        "colab_type": "text"
      },
      "source": [
        "**FINDING OBSERVATIONS IN EACH CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1URb8UBIKI3J",
        "colab_type": "code",
        "outputId": "e956f630-948e-4d0f-937d-976f03c0d5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df=train_data.groupby(train_data.sentiment_class)['original_text'].nunique()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment_class\n",
              "-1     769\n",
              " 0    1701\n",
              " 1     765\n",
              "Name: original_text, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZrIOkhMJAm9",
        "colab_type": "code",
        "outputId": "b7764d4e-aa5e-45d2-fb9d-f39eb8262888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df=df1[\"sentiment_class\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment_class\n",
              "-1     769\n",
              " 0    1701\n",
              " 1     765\n",
              "Name: original_text, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swaavv7e3St5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_for_dict=train_data['original_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l--AqIxmNRo",
        "colab_type": "code",
        "outputId": "8683539d-2ae9-4da5-f355-3906b6dc8927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'original_text', 'lang', 'retweet_count', 'original_author',\n",
              "       'sentiment_class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogEheJRA_ohS",
        "colab_type": "text"
      },
      "source": [
        "**CREATING A DICTIONARY MAPPING FROM EACH TO ITS TOTAL NUMBER OF APPEARANCE IN THE TEXT CORPUS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OWkr2-CEgSz",
        "colab_type": "text"
      },
      "source": [
        "This is necessary for encoding the word data by using their frequency count in the corpus because LSTM network used below can work with data encoded in the form of integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYX9AYZb3m9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_for_dict_1=[x.split() for x in train_data_for_dict]\n",
        "word_to_dict={}\n",
        "\n",
        "total_words=[]\n",
        "for x in train_data_for_dict_1:\n",
        "  total_words+=x\n",
        "\n",
        "for x in total_words:\n",
        "    word_to_dict[x]=total_words.count(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMSv7W1QDG0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_X=[]\n",
        "train_data_X=train_data_for_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeAmVhgzAX8a",
        "colab_type": "text"
      },
      "source": [
        "**FOLLOWING SET OF STATEMENTS WORK ON PULLING OUT ONLY ENGLISH ALPHABETS FROM THE CORPUS USING REGEX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzZBO_LLDI4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=list(train_data_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nzCRArDS8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_train=' '.join(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-BJwlK1DmXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_train=final_train.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6D0BKSnJCzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def MatchOnlyAlpha(text):\n",
        "  return re.findall(r'[a-z]+',text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXLjQrTrXU6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordlist=MatchOnlyAlpha(final_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-R2j-7zAmf9",
        "colab_type": "text"
      },
      "source": [
        "**TO FIND ALL THE UNIQUE WORDS IN THE CORPUS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4f4lqu4IwUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=sorted(list(set(wordlist)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WozoSfCQBEyK",
        "colab_type": "text"
      },
      "source": [
        "**TO FILTER OUT STOPWORDS LIKE \"THE ,A , AND ...\"**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFENiuEbjHFA",
        "colab_type": "code",
        "outputId": "b7b481fb-1da0-4301-eae3-d8948d49ca6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")#MOST COMMON STOPWORDS IN ENGLISH LANGUAGE CAN BE FOUNG IN NLTK'S STOPWORD COLLECTION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RytNoYGg7Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words('english'))\n",
        "words_final=[w for w in words if not w in stop_words] #WORDS , EXCLUDING STOP WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHamBwBTZZgE",
        "colab_type": "code",
        "outputId": "a5ef82f0-18db-4c4e-f450-35bb1a1978dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "start=time.time()\n",
        "wordfreq=[words_final.count(w) for w in words_final]\n",
        "elap=time.time()-start\n",
        "list_word2count=list(zip(words_final,wordfreq))\n",
        "print('Time taken:',elap) #TO POINT OUT THAT LIST COMPREHENSION IS FASTER THAN EXPLICIT FOR LOOPS :)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken: 4.016927242279053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg-RgoVzaoiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_word2count=dict(list_word2count) #FINAL WORD COLLECTION ALONG WITH THEIR COUNTS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMJVm3-Qz269",
        "colab_type": "code",
        "outputId": "29a22a06-9859-43a1-869b-6204ed79e5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(list_word2count)#TOTAL NUMBER OF WORDS(AFTER FILTERING)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWPt5MPCBPf",
        "colab_type": "text"
      },
      "source": [
        "**TO SORT FREQUENTLY OCCURING WORDS IN DESCENDING ORDER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKxuoFvPfBnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SortFreqDict(freqdict):\n",
        "  aux=[(freqdict[key],key) for key in freqdict]\n",
        "  aux.sort()\n",
        "  aux.reverse()\n",
        "  return aux"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3BTeiDxggP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vocabulary defined!\n",
        "sorteddict=SortFreqDict(list_word2count) #SORTED WORD DICTIONARY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbJWidVMkd85",
        "colab_type": "code",
        "outputId": "44fde45f-ee01-4754-cb74-013043d6c06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_vocab=len(sorteddict)\n",
        "n_words=len(wordlist)\n",
        "print('No. of vocab words: ',n_vocab) #AFTER FILTERING\n",
        "print('No. of words: ',n_words)#WITHOUT FILTERING"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab words:  15441\n",
            "No. of words:  126776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erjf6wVhTxYx",
        "colab_type": "code",
        "outputId": "347ef798-b2c1-4788-be69-7f97749cb4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train_data_X[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Happy #MothersDay to all you amazing mothers o...\n",
              "1    Happy Mothers Day Mum - I'm sorry I can't be t...\n",
              "2    Happy mothers day To all This doing a mothers ...\n",
              "3    Happy mothers day to this beautiful woman...ro...\n",
              "4    Remembering the 3 most amazing ladies who made...\n",
              "Name: original_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybaba-jTCdS3",
        "colab_type": "text"
      },
      "source": [
        "**SPLITTING INTO TRAIN AND TEST SETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkFNsF90pSMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "random_indices=np.random.randint(0,len(train_data_X),len(train_data_X))\n",
        "\n",
        "train_data_X=[train_data_X[i] for i in random_indices]\n",
        "train_data_Y=[dataY[i] for i in random_indices]\n",
        "\n",
        "test_size=int(0.2*(len(train_data_X)))\n",
        "\n",
        "test_data_X=train_data_X[:test_size]\n",
        "test_data_Y=train_data_Y[:test_size]\n",
        "\n",
        "train_data_X=train_data_X[test_size:]\n",
        "train_data_Y=train_data_Y[test_size:]\n",
        "\n",
        "assert(len(train_data_X)==len(train_data_Y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgx9Ggyu_Zi",
        "colab_type": "text"
      },
      "source": [
        "**OPTIONAL: You can do data augmentation to the existing corpus to get generalisation in the model perfomance (on unseen data). The augmentation technique used below is to generate text samples with shuffled sentences. The intuition behind this is that , even when the order of sentences is shuffled , the ultimate sentiment conveyed is the same. I've set this step as optional because , this technique didn't produce better results than without augmenting data. This can produce better results if the number of samples are even larger than the provided data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wie_Z9lqOBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    '''text: list of text documents'''\n",
        "    tokenized =  sent_tokenize(text)\n",
        "    return tokenized\n",
        "\n",
        "def shuffle_tokenized(text):\n",
        "    random.shuffle(text)\n",
        "    newl=list(text)\n",
        "    shuffled.append(newl)\n",
        "    return text\n",
        "\n",
        "trainX=train_data_X\n",
        "testX=test_data_X\n",
        "\n",
        "testY=[]\n",
        "trainY=[]\n",
        "\n",
        "shuffled=[]\n",
        "augmented = []\n",
        "reps=[]\n",
        "\n",
        "for i,ng_rev in enumerate(trainX):\n",
        "    tok = tokenize(ng_rev)\n",
        "    shuffled= [tok]\n",
        "    label=train_data_Y[i]\n",
        "    for i in range(11):\n",
        "\t#generate 11 new tweets\n",
        "        shuffle_tokenized(shuffled[-1])\n",
        "    for k in shuffled:\n",
        "        s = ' '\n",
        "        new_rev = s.join(k)\n",
        "        if new_rev not in augmented:\n",
        "            augmented.append(new_rev)\n",
        "            trainY.append(label)\n",
        "        else:\n",
        "            reps.append(new_rev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9mCaJ5K11IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_Y_new=testY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmEBHEsn2pPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_Y_new=trainY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm1BspTCujKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_X_new=augmented #DO after running the above loop with trainX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtXoWpI4uuM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_X_new=augmented ##DO after running the above loop with testX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlwKORnXEYWj",
        "colab_type": "text"
      },
      "source": [
        "**ENCODING AUGMENTED DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAUAs02TyiSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X=[]\n",
        "for indx,valx in enumerate(train_data_X_new):\n",
        "  lst=[]\n",
        "  for indy,valy in enumerate(valx.split()):\n",
        "    try:\n",
        "      lst.append(word_to_dict[valy])\n",
        "    except:\n",
        "      lst.append(0)\n",
        "  train_X.append(lst)\n",
        "\n",
        "test_X=[]\n",
        "for indx,valx in enumerate(test_data_X_new):\n",
        "  lst=[]\n",
        "  for indy,valy in enumerate(valx.split()):\n",
        "    try:\n",
        "      lst.append(word_to_dict[valy])\n",
        "    except:\n",
        "      lst.append(0)\n",
        "  test_X.append(lst)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esd7ygK8D6uc",
        "colab_type": "text"
      },
      "source": [
        "**TESTING nltk.sent_tokenize()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUYvFI83rXQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nadH60hsq60b",
        "colab_type": "code",
        "outputId": "687bd77b-63ec-4249-ec47-0abce718b092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "strr=\"I am good\\ I am bad\\ I am large\\ I am glad\"\n",
        "token=sent_tokenize(strr)\n",
        "token"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am good\\\\ I am bad\\\\ I am large\\\\ I am glad']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoChn_gnE7o1",
        "colab_type": "text"
      },
      "source": [
        "**PREPARING DATA TO BE IN THE FORM REQUIRED BY THE LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjMq97_KlPz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now , it's time to prepare the data for training.\n",
        "max_tweet_len=500\n",
        "X_train=sequence.pad_sequences(train_X,maxlen=max_tweet_len)\n",
        "X_test=sequence.pad_sequences(test_X,maxlen=max_tweet_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGmJVXQYux5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict={1:0,-1:1,0:2}\n",
        "train_Y=[]\n",
        "for inx,valx in enumerate(train_data_Y_new):\n",
        "  train_Y.append(label_dict[valx])\n",
        "  \n",
        "test_Y=[]\n",
        "for iny,valy in enumerate(test_data_Y_new):\n",
        "  test_Y.append(label_dict[valy])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYFCVxVv9kzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ONE HOT ENCODING CLASS LABELS\n",
        "from keras.utils import to_categorical\n",
        "train_Y=np.array(train_Y)\n",
        "test_Y=np.array(test_Y)\n",
        "train_Y=to_categorical(train_Y,num_classes=3)\n",
        "test_Y=to_categorical(test_Y,num_classes=3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPjbSt0VFKOf",
        "colab_type": "text"
      },
      "source": [
        "**BELOW CODE SNIPPET IMPLEMENT A CLASS FOR REALISING STOCHASTIC WEIGHT AVERAGING ( A METHOD TO HAVE A BETTER TEST INFERENCE )**\n",
        "For more details on it , refer [Stochastic Weight Averaging article](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1TR8ePQPol_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import keras.callbacks as callbacks\n",
        "\n",
        "class SnapshotCallbackBuilder:\n",
        "    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n",
        "        self.T = nb_epochs\n",
        "        self.M = nb_snapshots\n",
        "        self.alpha_zero = init_lr\n",
        "\n",
        "    def get_callbacks(self, model_prefix='Model'):\n",
        "\t\n",
        "\t\n",
        "        callback_list = [\n",
        "            callbacks.ModelCheckpoint(\"model_best.h5\",monitor='val_loss', \n",
        "                                   mode = 'min', save_best_only=True, verbose=1),\n",
        "            swa,\n",
        "            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n",
        "        ]\n",
        "\n",
        "        return callback_list\n",
        "\n",
        "    \n",
        "    def _cosine_anneal_schedule(self, t):\n",
        "        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n",
        "        cos_inner /= self.T // self.M\n",
        "        cos_out = np.cos(cos_inner) + 1\n",
        "        return float(self.alpha_zero / 2 * cos_out)\n",
        "\n",
        "class SWA(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, filepath, swa_epoch):\n",
        "        super(SWA, self).__init__()\n",
        "        self.filepath = filepath\n",
        "        self.swa_epoch = swa_epoch \n",
        "    \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.nb_epoch = self.params['epochs']\n",
        "        print('Stochastic weight averaging selected for last {} epochs.'\n",
        "              .format(self.nb_epoch - self.swa_epoch))\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        if epoch == self.swa_epoch:\n",
        "            self.swa_weights = self.model.get_weights()\n",
        "            \n",
        "        elif epoch > self.swa_epoch:    \n",
        "            for i in range(len(self.swa_weights)):\n",
        "                self.swa_weights[i] = (self.swa_weights[i] * \n",
        "                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        self.model.set_weights(self.swa_weights)\n",
        "        print('Final model parameters set to stochastic weight average.')\n",
        "        self.model.save_weights(self.filepath)\n",
        "        print('Final stochastic averaged weights saved to file.')\n",
        "\n",
        "epochs=15 # Needed to be specified , to make the SWA object know for how many epochs the weights averaging is to be applied\n",
        "\n",
        "swa = SWA('keras_swa.h5',epochs-3)\n",
        "\n",
        "snapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naFYmL-eGEhH",
        "colab_type": "text"
      },
      "source": [
        "**LSTM MODEL ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6afwFzJLGC6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model starts!\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Conv1D,Embedding\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Embedding(n_vocab,32,input_length=500))\n",
        "model.add(Conv1D(32,3,padding='same',activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\n",
        "model.add(LSTM(100,dropout=0.4,recurrent_dropout=0.2,return_sequences=False))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='Adam')\n",
        "plot_model(model,to_file='LSTM_SWA.png',show_shapes=True,show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7gcWLqIGJAW",
        "colab_type": "text"
      },
      "source": [
        "**MODEL TRAINING :**\n",
        "\n",
        "   epochs : 15\n",
        "   \n",
        "   batch_size : 64\n",
        "\n",
        "   learning_rate_initial : 1e-3 ( Cosine Annealing schedule used to reduce  learning_rate as training progresses)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzNTD7gdURwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H=model.fit(X_train,train_Y,validation_data=(X_test,test_Y),batch_size=64,epochs=epochs,callbacks=snapshot.get_callbacks())\n",
        "\n",
        "# To load the stochastic weights \n",
        "try:\n",
        "    model.load_weights('keras_swa.h5')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "#Save model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqd501e9G1qK",
        "colab_type": "text"
      },
      "source": [
        "**STEPS TO MAKE SUBMISSION FOR CONTEST , ALSO TO MAKE A TEST INFERENCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3NSp3HvBebv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_path=\"\" #Insert your own test data path here\n",
        "test_data=pd.read_csv(test_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thON3wFHZuk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_X=test_data['original_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o7JbsLppaQWa",
        "colab": {}
      },
      "source": [
        "test_data_X=list(test_data_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vey4QrydbDwD",
        "colab": {}
      },
      "source": [
        "wordlist_test=[MatchOnlyAlpha(x.lower()) for x in test_data_X if x not in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MxJNyIygbDwK",
        "colab": {}
      },
      "source": [
        "testing_X=[]\n",
        "for indx,valx in enumerate(wordlist_test):\n",
        "  lst=[]\n",
        "  for indy,valy in enumerate(valx):\n",
        "    try:\n",
        "      lst.append(word_to_dict[valy])\n",
        "    except:\n",
        "      lst.append(1)\n",
        "  testing_X.append(lst)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tEdg6rpbDwN",
        "colab": {}
      },
      "source": [
        "#Now , it's time to prepare the data for testing.\n",
        "max_tweet_len=500\n",
        "testing_X=sequence.pad_sequences(testing_X,maxlen=max_tweet_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQZIqoHBcvoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=model.predict(testing_X) # You can have your own testing_X to make a test inference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZQs0BaTcyVf",
        "colab_type": "code",
        "outputId": "f7200d7e-fc3b-4f25-ea83-e89095a1b23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1387, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omX-syBOc1Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=np.argmax(preds,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8s0mSs7dB-b",
        "colab_type": "code",
        "outputId": "536ae2ef-6059-4946-c2cf-8b028ee0bfd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1387,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXyp3jTZdC1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs=[]\n",
        "for i,x in enumerate(list(preds)):\n",
        "  if(x==0):\n",
        "    outputs.append(1)\n",
        "  elif(x==1):\n",
        "    outputs.append(-1)\n",
        "  else:\n",
        "    outputs.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApCH0FDEdFbH",
        "colab_type": "code",
        "outputId": "8aaf9da7-4ce2-499f-b34f-28f0cb4ca5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pvotTUqddQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_result=pd.Series(outputs,name=\"sentiment_class\")\n",
        "\n",
        "table=pd.concat([pd.Series(test_data[\"id\"],name=\"id\"),test_result],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMIxz2TzeByA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table=table.to_csv('Test_results.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP9eZQZvHLyx",
        "colab_type": "text"
      },
      "source": [
        "**There are various ways to extend this work :**\n",
        "\n",
        " 1. Use more classifiers like KNN, Multinomial Naive Bayes classifier and have a encompassing soft voting classifier on the results of the constiuent classifiers including the above neural network\n",
        "\n",
        "2. Use better data augmentation techniques\n",
        "\n",
        "3. Use better data encoding techniques like TfifdVectoriser , HashVectorizer etc."
      ]
    }
  ]
}